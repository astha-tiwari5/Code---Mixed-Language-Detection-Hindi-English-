# Code-Mixed Language Detection (Hindiâ€“English)

## ğŸ“Œ Overview
Code-mixed text, especially Hindiâ€“English (Hinglish), is very common in social media, chat applications, and informal communication in multilingual societies like India. Traditional language identification systems fail on such data because they operate at the sentence level.

This project implements **token-level language identification** for **Hindiâ€“English code-mixed text** using a **multilingual Transformer model (XLM-RoBERTa)**. Each word in a sentence is classified as either **Hindi (HI)** or **English (EN)**.

---

## ğŸ¯ Objectives
- Perform **word-level language detection** on code-mixed sentences
- Handle both **Devanagari Hindi** and **English**
- Demonstrate Transformer-based token classification
- Provide an interactive **Streamlit web application**

---

## ğŸ§  Approach
- Treat language identification as a **token classification** problem
- Use **XLM-RoBERTa**, a pretrained multilingual Transformer
- Align word-level labels with subword tokenization
- Perform inference without fine-tuning (baseline experiment)

---

## ğŸ—ï¸ Project Architecture

Input Sentence
â†“
Word Tokenization
â†“
XLM-RoBERTa Tokenizer
â†“
Subword Embeddings
â†“
Token Classification Head
â†“
Word-level Language Labels (HI / EN)



---

## ğŸ“‚ Project Structure

code-mixed-language-detection/
â”‚
â”œâ”€â”€ app.py # Streamlit web app
â”œâ”€â”€ notebook.ipynb # Experiments and evaluation
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ README.md # Project documentation
â””â”€â”€ .gitignore


---

## ğŸ§ª Dataset
A small manually annotated Sentence: à¤†à¤œ meeting à¤¥à¥€ boss à¤•à¥‡ à¤¸à¤¾à¤¥
Tokens: à¤†à¤œ | meeting | à¤¥à¥€ | boss | à¤•à¥‡ | à¤¸à¤¾à¤¥
Labels: HI | EN | HI | EN | HI | HI


> âš ï¸ Note: The dataset is intentionally small and used only for baseline evaluation.

à¤†à¤œ meeting à¤¥à¥€ boss à¤•à¥‡ à¤¸à¤¾à¤¥
à¤†à¤œ â†’ HI
meeting â†’ EN
à¤¥à¥€ â†’ HI
boss â†’ EN
à¤•à¥‡ â†’ HI
à¤¸à¤¾à¤¥ â†’ HI


ğŸ“Š Evaluation

Evaluation is performed using precision, recall, and F1-score.

precision    recall  f1-score
EN    0.30      1.00     0.46
HI    0.00      0.00     0.00

âš ï¸ Limitations

Model is not fine-tuned on Hinglish data
Romanized Hindi (e.g., kal, tum, kaha) is often misclassified
Very small dataset
Script bias toward English

ğŸ”® Future Work

Fine-tune XLM-RoBERTa on large Hinglish datasets
Add CRF layer for better sequence consistency
Extend to multi-language code-mixing
Support Romanized Hindi
Integrate sentence-level aggregation

ğŸ§ª Technologies Used

Python
PyTorch
HuggingFace Transformers
XLM-RoBERTa
Scikit-learn
Streamlit
